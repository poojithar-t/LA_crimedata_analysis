# -*- coding: utf-8 -*-
"""Python _project_group 35.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11Mbo5JdMuDmp6HVBhbXN-VbxV93DfXCd

# Crime rate analysis in Los Angeles using predictive modeling and visualization

# Part-1: Database tables

Export data as csv from 
https://data.lacity.org/Public-Safety/Crime-Data-from-2020-to-Present/2nrs-mtv8
"""

import pandas as pd
import sqlite3
from sqlite3 import Error

def create_connection(db_file, delete_db=False):
    import os
    if delete_db and os.path.exists(db_file):
        os.remove(db_file)

    conn = None
    try:
        conn = sqlite3.connect(db_file)
        conn.execute("PRAGMA foreign_keys = 1")
    except Error as e:
        print(e)

    return conn


def create_table(conn, create_table_sql, drop_table_name=None):
    
    if drop_table_name:
        try:
            c = conn.cursor()
            c.execute("""DROP TABLE IF EXISTS %s""" % (drop_table_name))
        except Error as e:
            print(e)
    
    try:
        c = conn.cursor()
        c.execute(create_table_sql)
    except Error as e:
        print(e)
        
def execute_sql_statement(sql_statement, conn):
    cur = conn.cursor()
    cur.execute(sql_statement)

    rows = cur.fetchall()

    return rows

normalized_database_filename = 'normalized.db'
conn = create_connection(normalized_database_filename, delete_db=True)

from datetime import datetime


def extract_crime_details(df):
  crime_details = []
  for index, series in df.iterrows():
    crime = (series['DR_NO'], series['Crm Cd'], series['Crm Cd Desc'], series['Weapon Used Cd'], series['Weapon Desc'])
    crime_details.append(crime)
  return crime_details


def extract_crime_dates(df):
  crime_dates = []
  for index, series in df.iterrows():
    date_reported = datetime.strptime(series['Date Rptd'].split(" ")[0], '%m/%d/%Y')
    date_occured = datetime.strptime(series['DATE OCC'].split(" ")[0], '%m/%d/%Y')
    date = (series['DR_NO'], date_reported, date_occured, series['TIME OCC'])
    crime_dates.append(date)
  return crime_dates


def extract_crime_locations(df):
  crime_locations = []
  for index, series in df.iterrows():
    location = (series['DR_NO'], series['AREA'], series['AREA NAME'], series['Rpt Dist No'], series['Premis Cd'], series['Premis Desc'], series['LOCATION'], series['Cross Street'], series['LAT'], series['LON'])
    crime_locations.append(location)
  return crime_locations


def extract_victim_data(df):
  victim_data = []
  for index, series in df.iterrows():
    if int(series['Vict Age']) == 0:
      continue
    victim = (series['DR_NO'], series['Vict Age'], series['Vict Sex'], series['Vict Descent'])
    victim_data.append(victim)
  return victim_data


def extract_crime_status(df):
  crime_status = []
  for index, series in df.iterrows():
    status = (series['DR_NO'], series['Status'], series['Status Desc'])
    crime_status.append(status)
  return crime_status


def extract_addt_crime_codes(df):
  crime_codes = []
  for index, series in df.iterrows():
    codes = (series['DR_NO'], series['Crm Cd'], series['Crm Cd 1'], series['Crm Cd 2'], series['Crm Cd 3'], series['Crm Cd 4'])
    crime_codes.append(codes)
  return crime_codes


create_table_crime_details = """
    CREATE TABLE CrimeDetails(
    [DR_NO] INTEGER NOT NULL PRIMARY KEY,
    [CrimeCode] INTEGER NOT NULL,
    [CrimeCodeDesc] TEXT NOT NULL,
    [Weapon] TEXT,
    [WeaponDesc] TEXT
    );
    """


create_table_crime_dates = """
    CREATE TABLE CrimeDates(
    [DR_NO] INTEGER NOT NULL PRIMARY KEY,
    [DateReported] DATE NOT NULL,
    [DateOccured] DATE NOT NULL,
    [TimeOccured] INTEGER NOT NULL
    );
    """


create_table_crime_locations = """
    CREATE TABLE CrimeLocations(
    [DR_NO] INTEGER NOT NULL PRIMARY KEY,
    [AreaCode] INTEGER NOT NULL,
    [AreaName] TEXT NOT NULL,
    [DistrictNumber] INTEGER NOT NULL,
    [PremiseCode] INTEGER,
    [PremiseDesc] TEXT,
    [Location] TEXT NOT NULL,
    [CrossStreet] TEXT,
    [Latitude] FLOAT NOT NULL,
    [Longitude] FLOAT NOT NULL
    );
    """


create_table_victim_data = """
    CREATE TABLE VictimData(
    [DR_NO] INTEGER NOT NULL PRIMARY KEY,
    [VictimAge] INTEGER,
    [VictimSex] TEXT,
    [VictimDescent] TEXT 
    );
    """


create_table_crime_status = """
    CREATE TABLE CrimeStatus(
    [DR_NO] INTEGER NOT NULL PRIMARY KEY,
    [Status] TEXT NOT NULL,
    [StatusDesc] TEXT NOT NULL
    );
    """


create_table_addt_crime_codes = """
    CREATE TABLE AdditionalCrimeCodes(
    [DR_NO] INTEGER NOT NULL PRIMARY KEY,
    [CrimeCode] INTEGER NOT NULL,
    [CrimeCode1] INTEGER,
    [CrimeCode2] INTEGER,
    [CrimeCode3] INTEGER,
    [CrimeCode4] INTEGER
    );
    """

# Export data as csv from https://data.lacity.org/Public-Safety/Crime-Data-from-2020-to-Present/2nrs-mtv8
# filename='/content/sample_data/Crime_Data_from_2020_to_Present.csv'
filename='/content/Crime_Data_from_2020_to_Present.csv'


# reading the csv file
df = pd.read_csv(filename)


create_table(conn,create_table_crime_details,'CrimeDetails')
crime_details = extract_crime_details(df)
with conn:
  cur = conn.cursor()
  cur.executemany("INSERT INTO CrimeDetails VALUES(?,?,?,?,?)", crime_details)


create_table(conn,create_table_crime_dates,'CrimeDates')
crime_dates = extract_crime_dates(df)
with conn:
  cur = conn.cursor()
  cur.executemany("INSERT INTO CrimeDates VALUES(?,?,?,?)", crime_dates)


create_table(conn,create_table_crime_locations,'CrimeLocations')
crime_locations = extract_crime_locations(df)
with conn:
  cur = conn.cursor()
  cur.executemany("INSERT INTO CrimeLocations VALUES(?,?,?,?,?,?,?,?,?,?)", crime_locations)


create_table(conn,create_table_victim_data,'VictimData')
victim_data = extract_victim_data(df)
with conn:
  cur = conn.cursor()
  cur.executemany("INSERT INTO VictimData VALUES(?,?,?,?)", victim_data)

create_table(conn,create_table_crime_status,'CrimeStatus')
crime_status = extract_crime_status(df)
with conn:
  cur = conn.cursor()
  cur.executemany("INSERT INTO CrimeStatus VALUES(?,?,?)", crime_status)


create_table(conn,create_table_addt_crime_codes,'AdditionalCrimeCodes')
crime_codes = extract_addt_crime_codes(df)
with conn:
  cur = conn.cursor()
  cur.executemany("INSERT INTO AdditionalCrimeCodes VALUES(?,?,?,?,?,?)", crime_codes)

"""- Table: CrimeDetails"""

display(pd.read_sql_query("SELECT * FROM CrimeDetails limit 5;",conn))

"""- Table: CrimeDates"""

display(pd.read_sql_query("SELECT * FROM CrimeDates limit 5;",conn))

"""- Table: CrimeLocations"""

display(pd.read_sql_query("SELECT * FROM CrimeLocations limit 5;",conn))

"""- Table: VictimData"""

display(pd.read_sql_query("SELECT * FROM VictimData limit 5;",conn))

"""- Table: CrimeStatus"""

display(pd.read_sql_query("SELECT * FROM CrimeStatus limit 5;",conn))

"""- Table: AdditionalCrimeCodes"""

display(pd.read_sql_query("SELECT * FROM AdditionalCrimeCodes limit 5;",conn))

"""# Part-2: Data Analysis

- Plotting
"""

types_of_crime = pd.read_sql_query("""SELECT count(*) AS CrimeCount, CrimeCodeDesc FROM CrimeDetails 
                                    GROUP BY CrimeCodeDesc 
                                    HAVING CrimeCount>30000 
                                    ORDER BY CrimeCount desc;""", conn)
types_of_crime.plot.barh(x='CrimeCodeDesc', y='CrimeCount', rot=0)

times_of_crime = pd.read_sql_query("""SELECT count(*) as CrimeCount, d.TimeOccured from CrimeDetails c
                                      INNER JOIN CrimeDates d ON c.DR_NO=d.DR_NO 
                                      GROUP BY d.TimeOccured 
                                      HAVING CrimeCount>10000 AND d.TimeOccured NOT IN (1, 800) 
                                      ORDER BY CrimeCount desc;""", conn)
times_of_crime.plot.barh(x='TimeOccured', y='CrimeCount', rot=0)

age_data = pd.read_sql_query("""SELECT count(*) AS Crime_Count, Victimdata.VictimAge FROM CrimeDetails
                                INNER JOIN VictimData ON CrimeDetails.DR_NO = VictimData.DR_NO 
                                WHERE VictimData.VictimAge BETWEEN 19 AND 45 
                                GROUP BY VictimData.VictimAge 
                                ORDER BY VictimData.VictimAge asc;""", conn)
age_data.plot.barh(x='VictimAge', y='Crime_Count', rot=0)

areas_data = pd.read_sql_query("""SELECT count(*) AS Crime_Count, CrimeLocations.AreaName AS Area FROM CrimeDetails 
                                  INNER JOIN CrimeLocations ON CrimeDetails.DR_NO = CrimeLocations.DR_NO 
                                  GROUP BY CrimeLocations.AreaName 
                                  ORDER BY Crime_Count desc LIMIT 5;""", conn)
areas_data.plot.barh(x='Area', y='Crime_Count', rot=0)

"""- Feature correlation"""

import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np


features = pd.read_sql_query("""SELECT c.CrimeCode, c.Weapon, d.TimeOccured, l.AreaCode, l.DistrictNumber, l.PremiseCode, v.VictimAge, v.VictimSex, v.VictimDescent FROM CrimeDetails c 
                                    INNER JOIN CrimeDates d on c.DR_NO=d.DR_NO
                                    INNER JOIN CrimeLocations l on d.DR_NO=l.DR_NO
                                    INNER JOIN VictimData v on l.DR_NO=v.DR_NO;""", conn)

# Converting victim descents to numeric
descents=['A', 'B', 'C', 'D', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'O', 'P', 'S', 'U', 'V', 'W', 'X', 'Z']
classes=list(range(0,len(descents)))
features['VictimDescent'].replace(descents, classes, inplace=True)

# Converting victim gender to numeric
genders=['F', 'M']
classes=list(range(0,len(genders)))
features['VictimSex'].replace(genders, classes, inplace=True)

# drop na and None values
features = features.replace(to_replace='None', value=np.nan).dropna()

# Pearson Correlation
plt.figure(figsize=(12,10))
cor = features.corr()
sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)
plt.show()
# display(features)

"""- Random forest classification

Selecting the crimes that can be considered for classification. We ignored the crimes that have a less count have only considered the most reported crimes. Only the crimes shown above (which have crime count > 30000) are considered for classification.
"""

types_of_crime = pd.read_sql_query("select count(*) as CrimeCount, CrimeCode, CrimeCodeDesc from CrimeDetails group by CrimeCode, CrimeCodeDesc having CrimeCount>30000 order by CrimeCount desc;", conn)
display(types_of_crime)

"""Extracting the data: CrimeCode, AreaCode, PremiseCode, TimeOccured, VictimAge, VictimDescent for classification."""

# Considering crime codes: 510, 624, 330, 740, 310, 354, 230, 626, 440
crime_df = pd.read_sql_query("select c.CrimeCode, l.AreaCode, l.PremiseCode, d.TimeOccured, v.VictimAge, v.VictimDescent from CrimeDetails c INNER JOIN CrimeLocations l on c.DR_NO=l.DR_NO INNER JOIN CrimeDates d on d.DR_NO=l.DR_NO INNER JOIN VictimData v on v.DR_NO=l.DR_NO where CrimeCode in ('510', '624', '330', '740', '310', '354', '230', '626', '440');", conn)

"""Pre-processing the dataframe by converting categorical data to numeric and removing NA values"""

# convert categorical to numeric 
descents=['A', 'B', 'C', 'D', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'O', 'P', 'S', 'U', 'V', 'W', 'X', 'Z']
classes=list(range(0,len(descents)))
crime_df['VictimDescent'].replace(descents, classes, inplace=True)

# drop na values
crime_df = crime_df.dropna()

display(crime_df)

"""Extracting features and label"""

import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import export_graphviz
import pydot

label = np.array(crime_df['CrimeCode'])
features = crime_df.drop('CrimeCode',axis = 1)
feature_list = list(features.columns)
features = np.array(features)
print("Predict the label: ", label)
print("Predicting on features: ",feature_list)

"""Splitting train and test data"""

X_train, X_test, y_train, y_test = train_test_split(features, label, test_size = 0.25)
print('Training Features:', X_train.shape)
print('Training Labels:', y_train.shape)
print('Testing Features:', X_test.shape)
print('Testing Labels:', y_test.shape)

"""Fit the training data into model, with 20 tress and max depth 4."""

random_forest = RandomForestRegressor(n_estimators = 20, max_depth = 4)
random_forest.fit(X_train, y_train);

"""Calculating the mean absolute error and accuracy."""

predict = random_forest.predict(X_test)
mean_error = abs(predict - y_test)
print('Mean Absolute Error: ', round(np.mean(mean_error), 2))
percent_error = 100 * (mean_error/y_test)
accuracy = 100 - np.mean(percent_error)
print('Accuracy:', round(accuracy, 2), '%')

"""# Part-3: Visualization

- Interactive 3D Map to visualize latitude, longitude and dates
"""

pip install Keplergl

from google.colab import output
from keplergl import KeplerGl

output.enable_custom_widget_manager()
coord_dataframe = pd.read_sql_query("SELECT d.DateOccured, l.Latitude, l.Longitude FROM CrimeLocations l JOIN CrimeDates d ON l.DR_NO=d.DR_NO;",conn)
kepler_map = KeplerGl(height = 800, data={'map_data': coord_dataframe})

# save the map to html file
kepler_map.save_to_html(file_name="kepler_map.html")

kepler_map